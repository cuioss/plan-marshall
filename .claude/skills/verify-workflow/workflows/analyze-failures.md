# Analyze Structural Failures Workflow

LLM-based analysis of structural verification failures. Runs conditionally when structural checks fail.

## Purpose

Transform simple pass/fail structural check results into actionable diagnostic reports with:
- Root cause categorization
- Origin tracing to test case definitions
- Targeted fix proposals

## Prerequisites

- Structural check results from `verify-structure.py`
- Test case files (`expected-artifacts.toon`, criteria files)
- Failure categories reference (`standards/failure-categories.md`)

## Step 1: Load Context

Read required files:

```
Read: {results_dir}/structural-checks.toon
Read: workflow-verification/test-cases/{test-id}/expected-artifacts.toon
Read: standards/failure-categories.md
```

Parse the structural results to extract failed checks.

## Step 2: For Each Failed Check

Iterate through each failure in the structural results.

### 2a. Extract Failure Details

From the structural results, extract:
- **check_name**: Name of the failed check
- **expected**: What was expected
- **actual**: What was found
- **error_message**: Specific error if provided

### 2b. Categorize Root Cause

Match the failure to one of the six categories from `standards/failure-categories.md`:

| Category | Match Criteria |
|----------|----------------|
| Missing Artifact | `actual` is empty, null, or "not found" |
| Schema Violation | Parse error, "invalid format", missing required field |
| Count Mismatch | `expected` and `actual` are numbers that differ |
| Scope Mismatch | Components list differs from expected |
| Reference Mismatch | Content differs from golden reference |
| Test Case Error | Test file parse error, invalid test-id |

### 2c. Trace Origin

Identify where the failure originated:

1. **If artifact-related**: Which component creates this artifact?
2. **If count-related**: Which component determines the count?
3. **If scope-related**: Which component selects scope?

Use the mapping from `standards/failure-categories.md` to identify likely origin.

### 2d. Generate Fix Proposal

Based on category, propose a targeted fix:

**Missing Artifact**:
```
Ensure {component} is invoked and completes successfully.
Check: Does the workflow include {component}? Is it receiving correct parameters?
```

**Schema Violation**:
```
Fix output format in {component}.
Issue: {specific_field_or_structure} is {invalid_state}.
```

**Count Mismatch**:
```
Review iteration logic in {component}.
Expected: {expected_count} | Actual: {actual_count}
Check: Scope filtering, loop conditions, or update test expectation if correct.
```

**Scope Mismatch**:
```
Review component selection in {component}.
Missing: {missing_components} | Extra: {extra_components}
Check: Module detection logic, domain filtering.
```

**Reference Mismatch**:
```
Compare {artifact} against golden reference.
Differences: {diff_summary}
Action: Update component output or update golden reference.
```

**Test Case Error**:
```
Fix test case definition.
File: {test_file}
Issue: {parsing_error_or_invalid_value}
```

## Step 3: Build Analysis TOON

Create structured output in TOON format:

```toon
analysis_status: analyzed
failure_count: {N}

failures[N]{check_name,category,origin,description,fix_proposal}:
{check_name},{category},{origin},{description},{fix_proposal}
```

Example:
```toon
analysis_status: analyzed
failure_count: 2

failures[2]{check_name,category,origin,description,fix_proposal}:
deliverable_count,Count Mismatch,phase-3-outline,Expected 5 deliverables but found 3,Review module detection in phase-3-outline; check domain filtering in config.toon
config_domains,Missing Artifact,phase-1-init,config.toon missing domains field,Ensure phase-1-init writes domains to config.toon during initialization
```

## Step 4: Write Analysis Output

Store analysis in the results directory:

```
Write: {results_dir}/structural-analysis.toon
```

Full file content:

```toon
# Structural Failure Analysis
# Generated by analyze-failures workflow

test_id: {test-id}
timestamp: {ISO timestamp}
analysis_status: analyzed
failure_count: {N}

failures[N]{check_name,category,origin,description,fix_proposal}:
{failure_1}
{failure_2}
...
```

## Output Schema

| Field | Type | Description |
|-------|------|-------------|
| `test_id` | string | Test case identifier |
| `timestamp` | ISO string | Analysis timestamp |
| `analysis_status` | enum | `analyzed` or `error` |
| `failure_count` | integer | Number of failures analyzed |
| `failures` | array | Structured failure records |

### Failure Record Fields

| Field | Description |
|-------|-------------|
| `check_name` | Name of the structural check that failed |
| `category` | One of six root cause categories |
| `origin` | Component likely responsible |
| `description` | Human-readable failure description |
| `fix_proposal` | Suggested fix action |

## Integration

This workflow is called by `test-and-verify.md` as Step V1.6 when Step V1 (structural verification) returns failures.
