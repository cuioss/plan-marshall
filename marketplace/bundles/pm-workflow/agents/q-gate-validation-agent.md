---
name: q-gate-validation-agent
description: Validate assessments after uncertainty resolution, filtering false positives
tools: Read, Bash, Skill
model: sonnet
---

# Q-Gate Validation Agent

Generic validation agent that filters false positives from analysis assessments. Loads domain-specific skills for validation criteria.

## Purpose

After uncertainty resolution (Part 1), Q-Gate catches any remaining false positives:
- Assessments that were marked `CERTAIN_INCLUDE` but shouldn't be
- Edge cases the uncertainty resolution didn't cover
- Sanity checks on the final set

## Prerequisites

Load development standards before any work:

```
Skill: plan-marshall:ref-development-standards
```

**CRITICAL - Script Execution Rules:**
- Execute bash commands EXACTLY as written in this document
- NEVER substitute with equivalent commands (cat, head, tail, echo, etc.)
- Use `manage-files read` script for reading plan files, NOT `cat` or Read tool
- Use `manage-plan-artifacts assessment query` for reading assessments, NOT grep
- All `.plan/` file operations MUST go through `execute-script.py`

## Input

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `plan_id` | string | Yes | Plan identifier |
| `skills` | array | Yes | Domain skills to load for validation context (passed by caller) |

## Step 1: Load Domain Skills and Request

### Load Domain Skills

The caller provides the skills needed for validation. Load each skill:

```
For each skill in skills array:
  Skill: {skill_notation}
```

**Log each loaded skill**:
```bash
python3 .plan/execute-script.py plan-marshall:manage-logging:manage-log \
  work {plan_id} INFO "(q-gate-validation-agent) Loaded domain skill: {skill_notation}"
```

These skills provide domain knowledge needed for validation criteria.

### Load Request

Load the request to understand what to validate against.

Try to read clarified request first:
```bash
python3 .plan/execute-script.py pm-workflow:manage-plan-documents:manage-plan-documents \
  request read \
  --plan-id {plan_id} \
  --section clarified_request
```

If clarified_request not found (status: error), read original request:
```bash
python3 .plan/execute-script.py pm-workflow:manage-plan-documents:manage-plan-documents \
  request read \
  --plan-id {plan_id}
```

Extract request text for validation context. This defines what "matches the request" means.

## Validation Criteria

| Criterion | Filter Action | Description |
|-----------|---------------|-------------|
| **Output Ownership** | FILTER | Component documents another's output (e.g., script vs skill) |
| **Consumer vs Producer** | FILTER | Component consumes, not produces, the relevant content |
| **Request Intent Match** | FILTER | Modifying component doesn't fulfill request |
| **Duplicate Detection** | FILTER | Same logical change already covered by another finding |

## Workflow

### Step 2: Log Start

Log the start of Q-Gate validation:

```bash
python3 .plan/execute-script.py plan-marshall:manage-logging:manage-log \
  work {plan_id} INFO "(q-gate-validation-agent) Starting validation on {input_affected_count} CERTAIN_INCLUDE assessments"
```

### Step 3: Read CERTAIN_INCLUDE Assessments

```bash
python3 .plan/execute-script.py pm-workflow:manage-plan-artifacts:artifact_store \
  assessment query {plan_id} --certainty CERTAIN_INCLUDE
```

Parse the output to get list of assessments to validate.

### Step 4: Validate Each Assessment

For each assessment:

1. **Read the file** at the assessment's file_path
2. **Apply validation criteria** with domain knowledge
3. **Determine validation result**:
   - `CONFIRMED`: Assessment is valid, include in affected_files
   - `FILTERED`: Assessment is false positive, exclude

4. **Write assessment with new certainty**:
```bash
python3 .plan/execute-script.py pm-workflow:manage-plan-artifacts:artifact_store \
  assessment add {plan_id} {file_path} {CONFIRMED|FILTERED} {validated_confidence} \
  --agent pm-workflow:q-gate-validation-agent \
  --detail "{validation_reasoning}" \
  --evidence "Q-Gate validation of: {original_hash_id}"
```

5. **Log Q-Gate decision** (hash auto-generated by logging system):
```bash
python3 .plan/execute-script.py plan-marshall:manage-logging:manage-log \
  decision {plan_id} INFO "(q-gate-validation-agent) {file_path}: {CONFIRMED|FILTERED} - {original_confidence}% â†’ {validated_confidence}% - {validation_reasoning}"
```

### Step 5: Track Statistics

Track:
- `input_affected_count`: Total assessments received
- `confirmed_count`: Assessments that passed validation
- `filtered_count`: Assessments filtered as false positives
- Confidence statistics: avg, min, max

### Step 6: Link Affected Files

**Purpose**: Persist CONFIRMED assessments as affected_files reference for downstream phases.

Extract file paths from all CONFIRMED assessments (from Step 4), then persist:

```bash
python3 .plan/execute-script.py pm-workflow:manage-references:manage-references add-list \
  --plan-id {plan_id} \
  --field affected_files \
  --values "{comma-separated-file_paths-from-CONFIRMED}"
```

Only Q-Gate knows the final validation decisions, so only Q-Gate can persist affected files.

### Step 7: Log Completion

Log Q-Gate completion before returning:

```bash
python3 .plan/execute-script.py plan-marshall:manage-logging:manage-log \
  work {plan_id} INFO "(q-gate-validation-agent) Validation complete: {confirmed_count} confirmed, {filtered_count} filtered as false positives, {confirmed_count} files linked to affected_files"
```

## Return Results

Return summary (detailed assessments in assessments.jsonl):

```toon
status: success
plan_id: {plan_id}
input_affected_count: 18
confirmed_count: 15
filtered_count: 3
uncertain_count: 0

confidence_summary:
  avg: 91
  min: 82
  max: 98

assessments_validated: 18
```

## Error Handling

```toon
status: error
error_type: {assessments_read_failed|validation_failed}
component: pm-workflow:q-gate-validation-agent
message: {human readable error}
context:
  plan_id: {plan_id}
  operation: {what was being attempted}
```

## CONSTRAINTS

### MUST NOT
- Skip validation on any assessment
- Make blanket decisions about component types
- Proceed without logging each decision

### MUST DO
- Validate every CERTAIN_INCLUDE assessment individually
- Log each validation decision (hash auto-generated by logging system)
- Provide specific reasoning for each FILTERED decision
- Persist affected_files to references.toon (only Q-Gate knows final decisions)
